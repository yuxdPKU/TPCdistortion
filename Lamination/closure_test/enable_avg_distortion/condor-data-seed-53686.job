# All local jobs are part of the vanilla universe.
Universe       = vanilla

# We want email if the job completed successfully. This can
# be set to Always, Error, or Never.
Notification   = Never

PeriodicHold   = (NumJobStarts>=1 && JobStatus == 1)

# Jobs by default get 1.4Gb of RAM allocated, ask for more if needed
# but if a job needs more than 2Gb it will not be able to run on the
# older nodes
request_memory = 4GB

# If you need multiple cores you can ask for them, but the scheduling
# may take longer the "larger" a job you ask for
request_cpus=1

# This flag is used to order only one's own submitted jobs 
# The jobs with the highest numbers get considered for 
# scheduling first.
Priority=20

# Copy all of the user's current shell environment variables 
# at the time of job submission.
GetEnv=True

# The requirement line specifies which machines we want to
# run this job on.  Any arbitrary classad expression can
# be used.
Requirements=(CPU_Speed >= 1)

# Rank is an expression that states how to rank machines which 
# have already met the requirements expression.  Essentially, 
# rank expresses preference.  A higher numeric value equals better 
# rank.  Condor will give the job the machine with the highest rank.
Rank=CPU_Speed

# Used to give jobs a directory with respect to file input 
# and output.
Initialdir     = /sphenix/u/xyu3/workarea/TPCdistortion/Lamination/closure_test/enable_avg_distortion

# The executable we want to run.
Executable     = $(Initialdir)/run_seeddst.sh

Nevent         = 100
RunNumber      = 53686
Segment        = $(Process)
ClusterFileName= DST_TRKR_CLUSTER_run2pp_ana489_2024p020_v001-$INT(RunNumber,%08d)-$INT(Segment,%05d).root
ClusterFilePath= /sphenix/lustre01/sphnxpro/production/run2pp/physics/ana489_2024p020_v001/DST_TRKR_CLUSTER/run_00053600_00053700/dst/
OutDir         = $(Initialdir)/
OutPrefix      = clusters_seeds
Index          = 0
StepSize       = 0

# The argument to pass to the executable.
#tracking firt then calo, only one tracking file
Arguments      = "$(Nevent) $(ClusterFileName) $(ClusterFilePath) $(OutDir) $(OutPrefix) $(Index) $(StepSize)"

# The job's stdout is sent to this file.
Output         = $(Initialdir)/log/job-Data-$(RunNumber).$(Segment).$(Index).$(Cluster).$(Process).out

# The job's stderr is sent to this file.
Error          = $(Initialdir)/log/job-Data-$(RunNumber).$(Segment).$(Index).$(Cluster).$(Process).err

# The condor log file for this job, useful when debugging.
Log            = $(Initialdir)/log/job-Data-$(RunNumber).$(Segment).$(Index).$(Cluster).$(Process).log

#should_transfer_files   = YES
#when_to_transfer_output = ON_EXIT_OR_EVICT
#transfer_output_files   = dummy.cc

on_exit_hold = (ExitBySignal == True) || (ExitCode != 0)

#Limiting the number of running jobs
#concurrency_limits=CONCURRENCY_LIMIT_DEFAULT:1000

# This should be the last command and tells condor to queue the
# job.  If a number is placed after the command (i.e. Queue 15)
# then the job will be submitted N times.  Use the $(Process)
# macro to make your input/output and log files unique.
#Queue Segment from $(Initialdir)/$(RunNumber)_seg.txt
Queue 100
